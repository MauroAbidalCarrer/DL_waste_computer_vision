{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7456329,"sourceType":"datasetVersion","datasetId":4340204}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Downloads and Imports","metadata":{}},{"cell_type":"code","source":"# Install KerasCV and PyCOCOTools metrics\n! pip install -q pycocotools git+https://github.com/keras-team/keras-cv@v0.6.4","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:30:06.588729Z","iopub.execute_input":"2024-01-24T16:30:06.589063Z","iopub.status.idle":"2024-01-24T16:30:35.655811Z","shell.execute_reply.started":"2024-01-24T16:30:06.589034Z","shell.execute_reply":"2024-01-24T16:30:35.654727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nprint(\"tensorflow:\", tf.__version__)\nimport keras_cv\nfrom keras_cv import bounding_box\nfrom keras_cv import visualization\nfrom keras.callbacks import LambdaCallback\nfrom tensorflow.keras.models import Model\n\nprint(\"keras_cv:\", keras_cv.__version__)\ndevice_name = tf.test.gpu_device_name()\n\nif \"GPU\" in device_name:\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    print('GPU not found.')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-24T16:30:35.658170Z","iopub.execute_input":"2024-01-24T16:30:35.658562Z","iopub.status.idle":"2024-01-24T16:30:53.088710Z","shell.execute_reply.started":"2024-01-24T16:30:35.658517Z","shell.execute_reply":"2024-01-24T16:30:53.087742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n    inputs = next(iter(inputs.take(1)))\n    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=value_range,\n        rows=rows,\n        cols=cols,\n        y_true=bounding_boxes,\n        scale=5,\n        font_scale=0.7,\n        bounding_box_format=bounding_box_format,\n        class_mapping=class_mapping,\n    )\n\ndef visualize_detections(model, dataset, bounding_box_format, rows=2, cols=2):\n    images, y_true = next(iter(dataset.take(1)))\n    y_pred = model.predict(images)\n    y_pred = bounding_box.to_ragged(y_pred)\n    visualization.plot_bounding_box_gallery(\n        images,\n        value_range=(0, 255),\n        bounding_box_format=bounding_box_format,\n        y_true=y_true,\n        y_pred=y_pred,\n        scale=4,\n        rows=rows,\n        cols=cols,\n        show=True,\n        font_scale=0.7,\n        class_mapping=class_mapping,\n    )\n    \ndef dict_to_tuple(inputs):\n    return inputs[\"images\"], bounding_box.to_dense(\n        inputs[\"bounding_boxes\"], max_boxes=32\n    )","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:30:53.089996Z","iopub.execute_input":"2024-01-24T16:30:53.090726Z","iopub.status.idle":"2024-01-24T16:30:53.100208Z","shell.execute_reply.started":"2024-01-24T16:30:53.090687Z","shell.execute_reply":"2024-01-24T16:30:53.099169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_mapping = {\n    1: \"Aluminium foil\",\n    2: \"Bottle\",\n    3: \"Bottle cap\",\n    4: \"Broken glass\",\n    5: \"Can\",\n    6: \"Carton\",\n    7: \"Cigarette\",\n    8: \"Cup\",\n    9: \"Lid\",\n    10: \"Other litter\",\n    11: \"Other plastic\",\n    12: \"Paper\",\n    13: \"Plastic bag - wrapper\",\n    14: \"Plastic container\",\n    15: \"Pop tab\",\n    16: \"Straw\",\n    17: \"Styrofoam piece\",\n    18: \"Unlabeled litter\",\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:30:53.102871Z","iopub.execute_input":"2024-01-24T16:30:53.103261Z","iopub.status.idle":"2024-01-24T16:30:53.121322Z","shell.execute_reply.started":"2024-01-24T16:30:53.103225Z","shell.execute_reply":"2024-01-24T16:30:53.120452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inspecting Data","metadata":{}},{"cell_type":"code","source":"train_tfrecord_file = '/kaggle/input/taco-tfrecord/Train_litter.tfrecord'\nval_tfrecord_file = '/kaggle/input/taco-tfrecord/Test_litter.tfrecord'\n\n# Create a TFRecordDataset\ntrain_dataset = tf.data.TFRecordDataset([train_tfrecord_file])\nval_dataset = tf.data.TFRecordDataset([val_tfrecord_file])\n\n# Iterate over a few entries and print their content. Uncomment this to look at the raw data\n# for record in train_dataset.take(1):\n#     example = tf.train.Example()\n#     example.ParseFromString(record.numpy())\n#     print(example)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:30:53.122659Z","iopub.execute_input":"2024-01-24T16:30:53.122923Z","iopub.status.idle":"2024-01-24T16:30:53.529019Z","shell.execute_reply.started":"2024-01-24T16:30:53.122900Z","shell.execute_reply":"2024-01-24T16:30:53.528069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_tfrecord_fn(example):\n    feature_description = {\n        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n        'image/height': tf.io.FixedLenFeature([], tf.int64),\n        'image/width': tf.io.FixedLenFeature([], tf.int64),\n        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n    }\n    \n    parsed_example = tf.io.parse_single_example(example, feature_description)\n\n    # Decode the JPEG image and normalize the pixel values to the [0, 1] range.\n    img = tf.image.decode_jpeg(parsed_example['image/encoded'], channels=3) # Returned as uint8\n    # Normalize the pixel values to [0, 256]\n    img = tf.image.convert_image_dtype(img, tf.uint8)\n\n    # Get the bounding box coordinates and class labels.\n    xmin = tf.sparse.to_dense(parsed_example['image/object/bbox/xmin'])\n    xmax = tf.sparse.to_dense(parsed_example['image/object/bbox/xmax'])\n    ymin = tf.sparse.to_dense(parsed_example['image/object/bbox/ymin'])\n    ymax = tf.sparse.to_dense(parsed_example['image/object/bbox/ymax'])\n    labels = tf.sparse.to_dense(parsed_example['image/object/class/label'])\n\n    # Stack the bounding box coordinates to create a [num_boxes, 4] tensor.\n    rel_boxes = tf.stack([xmin, ymin, xmax, ymax], axis=-1)\n    boxes = keras_cv.bounding_box.convert_format(rel_boxes, source='rel_xyxy', target='xyxy', images=img)\n\n    # Create the final dictionary.\n    image_dataset = {\n        'images': img,\n        'bounding_boxes': {\n            'classes': labels,\n            'boxes': boxes\n        }\n    }\n\n    return image_dataset\n\ntrain_dataset = train_dataset.map(parse_tfrecord_fn)\nval_dataset = val_dataset.map(parse_tfrecord_fn)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:30:53.530427Z","iopub.execute_input":"2024-01-24T16:30:53.531131Z","iopub.status.idle":"2024-01-24T16:30:55.302567Z","shell.execute_reply.started":"2024-01-24T16:30:53.531092Z","shell.execute_reply":"2024-01-24T16:30:55.301137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Batching\nBATCH_SIZE = 32\n# Adding autotune for pre-fetching\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n# Other constants\nNUM_ROWS = 4\nNUM_COLS = 8\nIMG_SIZE = 416\nBBOX_FORMAT = \"xyxy\"\n\ntrain_dataset = train_dataset.ragged_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nval_dataset = val_dataset.ragged_batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n\naugmenter = keras.Sequential(\n    [\n        keras_cv.layers.JitteredResize(\n            target_size=(IMG_SIZE, IMG_SIZE), scale_factor=(0.8, 1.25), bounding_box_format=BBOX_FORMAT\n        ),\n        keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\", bounding_box_format=BBOX_FORMAT),\n        keras_cv.layers.RandomRotation(factor=0.25, bounding_box_format=BBOX_FORMAT),\n        keras_cv.layers.RandomSaturation(factor=(0.4, 0.6)),\n        keras_cv.layers.RandomHue(factor=0.2, value_range=[0,255])\n    ]\n)\n\ntrain_dataset = train_dataset.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Resize and pad images\ninference_resizing = keras_cv.layers.Resizing(\n    IMG_SIZE, IMG_SIZE, pad_to_aspect_ratio=True, bounding_box_format=BBOX_FORMAT\n)\n\nval_dataset = val_dataset.map(inference_resizing, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:30:55.304349Z","iopub.execute_input":"2024-01-24T16:30:55.304833Z","iopub.status.idle":"2024-01-24T16:31:06.255349Z","shell.execute_reply.started":"2024-01-24T16:30:55.304794Z","shell.execute_reply":"2024-01-24T16:31:06.254582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Our Dataset ","metadata":{}},{"cell_type":"markdown","source":"### Training Data","metadata":{}},{"cell_type":"code","source":"# Visualize training set\nvisualize_dataset(\n    train_dataset, bounding_box_format=BBOX_FORMAT, value_range=(0, 255), rows=NUM_ROWS, cols=NUM_COLS\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:31:06.256586Z","iopub.execute_input":"2024-01-24T16:31:06.256874Z","iopub.status.idle":"2024-01-24T16:31:19.221171Z","shell.execute_reply.started":"2024-01-24T16:31:06.256849Z","shell.execute_reply":"2024-01-24T16:31:19.219558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation Data","metadata":{}},{"cell_type":"code","source":"# Visualize validation set\nvisualize_dataset(\n    val_dataset, bounding_box_format=BBOX_FORMAT, value_range=(0, 255), rows=NUM_ROWS, cols=NUM_COLS\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:31:19.223109Z","iopub.execute_input":"2024-01-24T16:31:19.223602Z","iopub.status.idle":"2024-01-24T16:31:28.057174Z","shell.execute_reply.started":"2024-01-24T16:31:19.223549Z","shell.execute_reply":"2024-01-24T16:31:28.055400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:31:28.062125Z","iopub.execute_input":"2024-01-24T16:31:28.062638Z","iopub.status.idle":"2024-01-24T16:31:28.150864Z","shell.execute_reply.started":"2024-01-24T16:31:28.062584Z","shell.execute_reply":"2024-01-24T16:31:28.149977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating RetinaNet Model and Doing Inference","metadata":{}},{"cell_type":"code","source":"base_lr = 0.0001\n# including a global_clipnorm is extremely important in object detection tasks\noptimizer_Adam = tf.keras.optimizers.Adam(\n    learning_rate=base_lr,\n    global_clipnorm=10.0\n)\n\ncoco_metrics = keras_cv.metrics.BoxCOCOMetrics(\n    bounding_box_format=BBOX_FORMAT, evaluate_freq=5\n)\n\nclass VisualizeDetections(keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if (epoch+1)%5==0:\n            visualize_detections(\n                self.model, bounding_box_format=BBOX_FORMAT, dataset=val_dataset, rows=NUM_ROWS, cols=NUM_COLS\n            )\n\ncheckpoint_path=\"best-custom-model\"\n\ncallbacks_list = [\n    # Conducting early stopping to stop after 6 epochs of non-improving validation loss\n    keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=6,\n    ),\n    \n    # Saving the best model\n    keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_path,\n        monitor=\"val_loss\",\n        mode=\"min\",\n        save_best_only=True,\n        save_weights_only=False,\n        save_freq='epoch',\n    ),\n    \n    # Custom metrics printing after each epoch\n    tf.keras.callbacks.LambdaCallback(\n    on_epoch_end=lambda epoch, logs: \n        print(f\"\\nEpoch #{epoch+1} \\n\" +\n              f\"Loss: {logs['loss']:.4f} \\n\" + \n              f\"mAP: {logs['MaP']:.4f} \\n\" + \n              f\"Validation Loss: {logs['val_loss']:.4f} \\n\" + \n              f\"Validation mAP: {logs['val_MaP']:.4f} \\n\") \n    ),\n    \n    # Visualizing results after each n epoch\n    VisualizeDetections()\n]\n\n# Building a RetinaNet model with a backbone trained on resnet50\ndef create_model():        \n    #back_bone_model = MobileNetV3Small(include_top=False, weights='imagenet')\n    model = keras_cv.models.RetinaNet.from_preset(\n        #\"yolo_v8_m_backbone_coco\",\n        #\"mobilenet_v3_small_imagenet\",\n        \"yolo_v8_xs_backbone_coco\",\n        num_classes=len(class_mapping),\n        bounding_box_format=BBOX_FORMAT\n    )\n    return model\n\nmodel = create_model()\n\n\n# Customizing non-max supression of model prediction.\nmodel.prediction_decoder = keras_cv.layers.MultiClassNonMaxSuppression(\n    bounding_box_format = BBOX_FORMAT,\n    from_logits=True,\n    iou_threshold=0.5,\n    confidence_threshold=0.5,\n)\n\n# Using focal classification loss and smoothl1 box loss with coco metrics\nmodel.compile(\n    classification_loss=\"focal\",\n    box_loss=\"smoothl1\",\n    optimizer=optimizer_Adam,\n    #metrics=[coco_metrics]\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T16:31:28.152064Z","iopub.execute_input":"2024-01-24T16:31:28.152351Z","iopub.status.idle":"2024-01-24T16:31:30.814272Z","shell.execute_reply.started":"2024-01-24T16:31:28.152325Z","shell.execute_reply":"2024-01-24T16:31:30.813520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=60,\n    #callbacks=callbacks_list,\n    verbose=1,\n)\nmodel.save(checkpoint_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Training and Validation Loss","metadata":{}},{"cell_type":"code","source":"# Access training and validation loss values from the history\ntraining_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\n\n# Plotting both training and validation loss\nepochs = range(1, len(training_loss) + 1)\n\nplt.plot(epochs, training_loss, label='Training Loss')\nplt.plot(epochs, validation_loss, label='Validation Loss')\n\nplt.title('Training and Validation Loss Over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the Best Model","metadata":{}},{"cell_type":"code","source":"from keras_cv.layers import MultiClassNonMaxSuppression\n# Import BoxCOCOMetrics from the correct module\nfrom keras_cv.metrics import BoxCOCOMetrics  # Replace 'your_module' with the actual module name\n\ncustom_objects = {\n    \"MultiClassNonMaxSuppression\": MultiClassNonMaxSuppression,\n    \"BoxCOCOMetrics\": lambda bounding_box_format=BBOX_FORMAT, evaluate_freq=5: BoxCOCOMetrics(bounding_box_format, evaluate_freq)\n    # include other custom objects if there are any\n}\n\nmodel = tf.keras.models.load_model(checkpoint_path, custom_objects=custom_objects, compile=False)\n\n# Using focal classification loss and smoothl1 box loss with coco metrics\nmodel.compile(\n    classification_loss=\"focal\",\n    box_loss=\"smoothl1\",\n    optimizer=optimizer_Adam,\n    #metrics=[coco_metrics]\n)\n# Visuaize on test set\nvisualize_detections(model, dataset=val_dataset.skip(1), bounding_box_format=BBOX_FORMAT, rows=NUM_ROWS, cols=NUM_COLS)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport keras_cv\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef visualize_boxes(image, true_boxes, pred_boxes, true_labels=None, pred_labels=None):\n    # Rescale the image if the pixel values are floats and fall outside [0, 1]\n    if image.dtype == np.float32 or image.dtype == np.float64:\n        if image.min() < 0 or image.max() > 1:\n            image = (image - image.min()) / (image.max() - image.min())\n\n    fig, ax = plt.subplots(1)\n    ax.imshow(image)\n\n    # True boxes in green\n    for box in true_boxes:\n        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='g', facecolor='none')\n        ax.add_patch(rect)\n        if true_labels is not None:\n            plt.text(box[0], box[1], true_labels, bbox=dict(facecolor='green', alpha=0.5))\n\n    # Predicted boxes in red\n    for box in pred_boxes:\n        rect = patches.Rectangle((box[1], box[0]), box[3] - box[1], box[2] - box[0], linewidth=2, edgecolor='r', facecolor='none')\n        ax.add_patch(rect)\n        if pred_labels is not None:\n            plt.text(box[0], box[1], pred_labels, bbox=dict(facecolor='red', alpha=0.5))\n\n    plt.show()\n\ndef calculate_errors_and_visualize(model, dataset, iou_threshold, confidence_threshold, bounding_box_format, debug=True):\n    error_counts = []\n\n    for images, y_true in dataset:  # Process one batch for demonstration\n        y_pred = model.predict(images)\n        y_pred = keras_cv.bounding_box.to_ragged(y_pred)\n\n        for image, true_boxes, true_classes, pred_boxes, pred_confidences, pred_classes in zip(images, y_true['boxes'], y_true['classes'], y_pred['boxes'], y_pred['confidence'], y_pred['classes']):\n            # Filter out boxes with class -1\n            valid_true_boxes = tf.boolean_mask(true_boxes, tf.not_equal(true_classes, -1))\n            valid_pred_boxes = tf.boolean_mask(pred_boxes, tf.not_equal(pred_classes, -1))\n            valid_pred_confidences = tf.boolean_mask(pred_confidences, tf.not_equal(pred_classes, -1))\n\n            # Further filter predicted boxes based on confidence threshold\n            confident_indices = tf.where(valid_pred_confidences > confidence_threshold)\n            pred_confident_boxes = tf.gather_nd(valid_pred_boxes, confident_indices)\n\n            # Compute IoU matrix\n            iou_matrix = keras_cv.bounding_box.compute_iou(valid_true_boxes, pred_confident_boxes, bounding_box_format)\n\n            # Handle cases where there are no confident predictions\n            if pred_confident_boxes.shape[0] == 0:\n                false_positives = 0\n                false_negatives = valid_true_boxes.shape[0]\n            else:\n                # Count false positives and false negatives\n                false_positives = np.sum(np.max(iou_matrix, axis=0) < iou_threshold)\n                false_negatives = np.sum(np.max(iou_matrix, axis=1) < iou_threshold)\n            error_counts.append(false_positives + false_negatives)\n\n            # Visualization\n            if false_positives + false_negatives > 0:\n                print(f\"Image: False Positives: {false_positives}, False Negatives: {false_negatives}\")\n                visualize_boxes(image.numpy(), valid_true_boxes.numpy(), pred_confident_boxes.numpy())\n                print()\n\n    return error_counts\n\n# Example usage:\nerror_counts = calculate_errors_and_visualize(model, train_dataset, iou_threshold=0.5, confidence_threshold=0.6, bounding_box_format=\"xyxy\", debug=True)\nprint(\"Total error counts for each image:\", error_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}